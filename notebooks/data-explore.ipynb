{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = [\n",
    "    \"../mydataset/FPS1024/test/FPS1024_0_0.tfrecords\",\n",
    "]\n",
    "tr_dataset = tf.data.TFRecordDataset(train_filenames).shuffle(1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(serialized_example, total_num_point):\n",
    "    features = tf.io.parse_example(\n",
    "        [serialized_example],\n",
    "        features={\n",
    "            \"xyz\": tf.io.FixedLenFeature([total_num_point, 3], tf.float32),\n",
    "            \"rgb\": tf.io.FixedLenFeature([total_num_point, 3], tf.float32),\n",
    "            \"translation\": tf.io.FixedLenFeature([3], tf.float32),\n",
    "            \"quaternion\": tf.io.FixedLenFeature([4], tf.float32),\n",
    "            \"num_valid_points_in_segment\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"seq_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"frame_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "            \"class_id\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        },\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "TOTAL_NUM_POINT = 1024\n",
    "\n",
    "tr_dataset = tr_dataset.map(lambda x: decode(x, TOTAL_NUM_POINT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element = iter(tr_dataset).get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg = tf.concat(values=[element.get(\"xyz\"), element.get(\"rgb\")], axis=0)\n",
    "seg = tf.squeeze(element.get(\"xyz\"))\n",
    "seg.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_matrix = tfgt.rotation_matrix_3d.from_quaternion(\n",
    "    element.get(\"quaternion\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten tensor to 1D array\n",
    "pos = tf.concat(\n",
    "    [\n",
    "        tf.reshape(rotation_matrix, -1),\n",
    "        tf.reshape(element.get(\"translation\"), -1),\n",
    "        tf.constant([0, 0, 0, 1], dtype=tf.float32),\n",
    "        tf.reshape(tf.cast(element.get(\"class_id\"), tf.float32), -1),\n",
    "    ],\n",
    "    axis=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../mydataset/my_train/0000_pos.npy\", pos.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../mydataset/my_train/0000_seg.npy\", seg.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read TFRecord in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from tfrecord.tools import tfrecord2idx\n",
    "from tfrecord.torch.dataset import TFRecordDataset\n",
    "\n",
    "DIR = \"../mydataset/FPS1024/test\"\n",
    "FILE_NAME = \"FPS1024_0_0.tfrecords\"\n",
    "FILE_PATH = os.path.join(DIR, FILE_NAME)\n",
    "index_path = FILE_PATH.replace(\"tfrecords\", \"idx\")\n",
    "description = {\n",
    "    \"xyz\": \"float\",\n",
    "    \"rgb\": \"float\",\n",
    "    \"translation\": \"float\",\n",
    "    \"quaternion\": \"float\",\n",
    "    \"num_valid_points_in_segment\": \"int\",\n",
    "    \"seq_id\": \"int\",\n",
    "    \"frame_id\": \"int\",\n",
    "    \"class_id\": \"int\",\n",
    "}\n",
    "\n",
    "dataset = TFRecordDataset(FILE_PATH, index_path, description)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
    "\n",
    "data = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d import transforms\n",
    "\n",
    "transforms.quaternion_to_matrix(data[\"quaternion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.quaternion_to_axis_angle(data[\"quaternion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "rotation_matrix = transforms.quaternion_to_matrix(data[\"quaternion\"])\n",
    "rotation_matrix = rotation_matrix[0, :, :]\n",
    "axag = cv2.Rodrigues(rotation_matrix.numpy())[0].flatten()\n",
    "axag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms.quaternion_to_axis_angle(data[\"quaternion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split `.tfrecord` files into train, test, and val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "ROOT_DIR = os.path.dirname(os.getcwd())\n",
    "DIR_NAME = \"mydataset/FPS1024\"\n",
    "\n",
    "FILE_PATHS = list(glob.iglob(os.path.join(ROOT_DIR, DIR_NAME, \"*.tfrecords\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(FILE_PATHS) == 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each tfrecord files have ~45000 elements, 50000 is enough for randomnization\n",
    "BUFFER_SIZE = 50000\n",
    "for f in FILE_PATHS:\n",
    "    dataset = tf.data.TFRecordDataset(f).shuffle(buffer_size=BUFFER_SIZE)\n",
    "\n",
    "    ds_size = len(list(dataset))\n",
    "    train_size, test_size, val_size = [int(ds_size * s) for s in [0.8, 0.1, 0.1]]\n",
    "\n",
    "    train_dataset = dataset.take(train_size)\n",
    "    test_dataset = dataset.skip(train_size)\n",
    "    val_dataset = test_dataset.skip(test_size)\n",
    "    test_dataset = test_dataset.take(test_size)\n",
    "\n",
    "    dir_path = os.path.join(ROOT_DIR, DIR_NAME)\n",
    "    for d, dataset in [\n",
    "        (\"train\", train_dataset),\n",
    "        (\"test\", test_dataset),\n",
    "        (\"val\", val_dataset),\n",
    "    ]:\n",
    "        file_path = os.path.join(dir_path, d, os.path.basename(f)[12:])\n",
    "        print(\"Saving\", file_path)\n",
    "        writer = tf.data.experimental.TFRecordWriter(file_path)\n",
    "        writer.write(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Index Path for all `.tfrecords` files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "FILE_PATHS = {\n",
    "    d: list(glob.iglob(os.path.join(ROOT_DIR, DIR_NAME, d, \"*.tfrecords\")))\n",
    "    for d in [\"train\", \"test\", \"val\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(FILE_PATHS) == 3\n",
    "assert len(FILE_PATHS.get(\"train\")) == 42\n",
    "assert len(FILE_PATHS.get(\"test\")) == 42\n",
    "assert len(FILE_PATHS.get(\"val\")) == 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.idx` file is required for multi-file loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paths in FILE_PATHS.values():\n",
    "    for path in paths:\n",
    "        idx_path = path.replace(\"tfrecords\", \"idx\")\n",
    "        tfrecord2idx.create_index(path, idx_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_PATHS = {\n",
    "    d: list(glob.iglob(os.path.join(ROOT_DIR, DIR_NAME, d, \"*.idx\")))\n",
    "    for d in [\"train\", \"test\", \"val\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure we have created `.idx` files for all `.tfrecord` files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(INDEX_PATHS) == 3\n",
    "assert len(INDEX_PATHS.get(\"train\")) == 42\n",
    "assert len(INDEX_PATHS.get(\"test\")) == 42\n",
    "assert len(INDEX_PATHS.get(\"val\")) == 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save those paths to a JSON file for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "get_filename = lambda x: pathlib.Path(x).stem\n",
    "\n",
    "filenames = map(get_filename, FILE_PATHS.get(\"train\"))\n",
    "# Convert map object to dict\n",
    "filenames_dict = {filename: 1 for filename in filenames}\n",
    "\n",
    "# Save to json\n",
    "json_path = os.path.join(ROOT_DIR, DIR_NAME, f\"FPS1024.json\")\n",
    "\n",
    "with open(json_path, \"w\") as f:\n",
    "    f.write(json.dumps(filenames_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's load `.tfrecords` files at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfrecord.torch.dataset import MultiTFRecordDataset\n",
    "\n",
    "tfrecord_pattern = os.path.join(ROOT_DIR, DIR_NAME, \"train/{}.tfrecords\")\n",
    "index_pattern = os.path.join(ROOT_DIR, DIR_NAME, \"train/{}.idx\")\n",
    "splits = json.load(open(json_path, \"r\"))\n",
    "\n",
    "\n",
    "dataset = MultiTFRecordDataset(\n",
    "    data_pattern=tfrecord_pattern,\n",
    "    index_pattern=index_pattern,\n",
    "    splits=splits,\n",
    "    description=description,\n",
    ")\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=32)\n",
    "\n",
    "data = next(iter(loader))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class myDatasetConfig(object):\n",
    "    def __init__(self):\n",
    "        self.num_class = 21\n",
    "\n",
    "        # Class name and id map\n",
    "        # see: https://github.com/GeeeG/CloudPose/blob/d6410dc4af9a58c00511e34fdc41c6cfd9f96cba/ycb_video_data_tfRecords/script/2_dataset_to_tfRecord_small.py\n",
    "        self.class_id = {\n",
    "            \"master chef can\": 0,\n",
    "            \"cracker box\": 1,\n",
    "            \"suger box\": 2,\n",
    "            \"tomato soup can\": 3,\n",
    "            \"mustard bottle\": 4,\n",
    "            \"tuna fish can\": 5,\n",
    "            \"pudding box\": 6,\n",
    "            \"gelatin box\": 7,\n",
    "            \"potted meat can\": 8,\n",
    "            \"banana\": 9,\n",
    "            \"pitcher base\": 10,\n",
    "            \"bleach cleanser\": 11,\n",
    "            \"bowl\": 12,\n",
    "            \"mug\": 13,\n",
    "            \"drill\": 14,\n",
    "            \"wood block\": 15,\n",
    "            \"scissors\": 16,\n",
    "            \"large marker\": 17,\n",
    "            \"large clapm\": 18,\n",
    "            \"extra large clamp\": 19,\n",
    "            \"foam brick\": 20,\n",
    "        }\n",
    "\n",
    "        self.id_class = {self.class_id[t]: t for t in self.class_id}\n",
    "\n",
    "        # 2D array\n",
    "        self.onehot_encoding = np.eye(self.num_class)[\n",
    "            np.array([range(self.num_class)]).reshape(-1)\n",
    "        ]\n",
    "\n",
    "    def sem2class(self, cls):\n",
    "        # Select ith row of the 2D array\n",
    "        onehot = self.onehot_encoding[int(cls), :]\n",
    "        return onehot\n",
    "\n",
    "    def size2class(self, class_name):\n",
    "        \"\"\"Convert 3D box size (l,w,h) to size class and size residual\"\"\"\n",
    "        size_class = self.class_id[class_name]  # 0\n",
    "        # size_residual = size - self.type_mean_size[type_name]  # 尺寸\n",
    "        return size_class\n",
    "\n",
    "    def class2size(self, pred_cls):\n",
    "        \"\"\"Inverse function to size2class\"\"\"\n",
    "        mean_size = self.type_mean_size[self.id_class[pred_cls]]\n",
    "        return mean_size\n",
    "\n",
    "    def class2sem(self, pred_cls):\n",
    "        \"\"\"Given point_cloud_with_cls, return class name\"\"\"\n",
    "        class_id = np.argwhere(pred_cls[0, -self.num_class :] == 1).flatten()[0]\n",
    "        return class_id\n",
    "\n",
    "    def id2class(self, cls):\n",
    "        \"\"\"Return class name given class id.\"\"\"\n",
    "        return self.id_class[cls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DC = myDatasetConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud = np.random.rand(1024, 3)\n",
    "one_hot = DC.sem2class(1)\n",
    "one_hot_ex_rep = np.repeat(np.expand_dims(one_hot, axis=0), 1024, axis=0)\n",
    "point_cloud_with_cls = np.concatenate((point_cloud, one_hot_ex_rep), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(point_cloud_with_cls[0, -21:] == 1).flatten()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DC.class2sem(point_cloud_with_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DC.id2class(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud_with_cls[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyntcloud import PyntCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mes = PyntCloud.from_file(\"../myDataset/ycb_video_obj_ply/002_master_chef_can.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh.plot(\n",
    "    return_scene=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "point_cloud = pd.DataFrame(np.random.rand(1024, 3), columns=[\"x\", \"y\", \"z\"])\n",
    "cloud = PyntCloud(point_cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('cloudpose')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3b22fb390b17f9ffe6c8d5453c7e9a4c026dd954b9700f0b8df11e79e074211"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
